---
title: О мелочах, которые бесят [1]
---

Вам не кажется, что поведение питоновского `itertools.groupby` совсем неинтуитивно? Ожидается, что функция `groupby()` буквально **group**s <iterable> **by** <key> — сгруппирует данные по ключу, но мы все знаем, что группировкой эта функция не занимается. Для последовательности `"AABBAAAB"` она сгенерирует 4 группы: по две на каждый ключ. Это работает за `O(n)`, это позволяет "группировать" данные из генераторов, хоть из бесконечных, на лету, но это не `groupby`! Это итератор по уже сгруппированным данным. Почему не `iterate_groups()` или что-нибудь в этом духе? 

Ладно, не мне учить отцов неймингу, но, согласитесь, вариант с "честной" группировкой на практике встречается куда чаще — в документации авторы и сами это признают:

> It generates a break or new group every time the value of the key function changes (which is why it is usually necessary to have sorted the data using the same key function). 

И при этом не дают нам отдельной функции, которая сделает всю группировку, а не половину. Хотя бы ключик (вроде -u в юниксовом uniq, который они приводят в доке в качестве примера) — но нет, ключика нам тоже не положено, поэтому в коде регулярно встречаются ужасно читаемые конструкции типа:

```python
groupby(sorted(sequence, key=some_key), key=some_key)
```

Кстати, очевидно, что сортировать за `O(n log n)` в определённых условиях хуже, чем завести `dict` для групп, пройтись за `O(n)` по последовательности, на каждом шаге за `O(1)` достать нужный элемент из `dict`'а (или за `O(1)` добавить туда `list` в котором будут элементы новой группы) и за `O(1)` вставить в этот `list` очередное значение.

Неэффективно по памяти? А какая разница, если на практике чаще всего мы просто сортируем в памяти? Не подходит для бесконечных генераторов? А когда вы в последний раз группировали данные из бесконечных генераторов?